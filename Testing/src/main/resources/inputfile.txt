These examples give a quick overview of the Spark API 
Spark is built on the concept of distributed datasets 
which contain arbitrary Java or Python objects 
You create a dataset from external data
then apply parallel operations to it 
The building block of the Spark API is its RDD API 
In the RDD API there are two types of operations: transformations 
which define a new dataset based on previous ones and actions 
which kick off a job to execute on a cluster On top of Spark’s RDD API 
high level APIs are provided eg DataFrame API and Machine Learning API 
These high level APIs provide a concise way to conduct certain data operations 
In this page we will show examples using RDD API as well as examples using high level APIs
n Spark a DataFrame is a distributed collection of data organized into named columns Users can use DataFrame API to perform various relational operations on both external data sources and Spark’s built-in distributed collections without providing specific procedures for processing data Also programs based on DataFrame API will be automatically optimized by Spark’s built-in optimizer Catalyst
MLlib Spark’s Machine Learning (ML) library provides many distributed ML algorithms These algorithms cover tasks such as feature extraction classification regression clustering recommendation and more MLlib also provides tools such as ML Pipelines for building workflows CrossValidator for tuning parameters and model persistence for saving and loading models
YARN and Hadoop Distributed File System (HDFS) are the cornerstone components of Hortonworks Data Platform (HDP) While HDFS provides the scalable fault-tolerant cost-efficient storage for your big data lake YARN provides the centralized architecture that enables you to process multiple workloads simultaneously YARN provides the resource management and pluggable architecture for enabling a wide variety of data access methods
HDP is the industry's only true secure enterprise-ready open source Apache™ Hadoop® distribution based on a centralized architecture (YARN) HDP addresses the complete needs of data-at-rest powers real-time customer applications and delivers robust analytics that accelerate decision making and innovation
Sometimes you will get an OutOfMemoryError not because your RDDs don’t fit in memory but because the working set of one of your tasks such as one of the reduce tasks in groupByKey was too large Spark’s shuffle operations (sortByKey groupByKey reduceByKey join etc) build a hash table within each task to perform the grouping which can often be large The simplest fix here is to increase the level of parallelism so that each task’s input set is smaller Spark can efficiently support tasks as short as 200 ms because it reuses one executor JVM across many tasks and it has a low task launching cost so you can safely increase the level of parallelism to more than the number of cores in your clusters
Using the broadcast functionality available in SparkContext can greatly reduce the size of each serialized task and the cost of launching a job over a cluster If your tasks use any large object from the driver program inside of them (eg a static lookup table) consider turning it into a broadcast variable Spark prints the serialized size of each task on the master so you can look at that to decide whether your tasks are too large; in general tasks larger than about 20 KB are probably worth optimizing
Data locality can have a major impact on the performance of Spark jobs If data and the code that operates on it are together then computation tends to be fast But if code and data are separated one must move to the other Typically it is faster to ship serialized code from place to place than a chunk of data because code size is much smaller than data Spark builds its scheduling around this general principle of data locality
Spark prefers to schedule all tasks at the best locality level but this is not always possible In situations where there is no unprocessed data on any idle executor Spark switches to lower locality levels There are two options: a) wait until a busy CPU frees up to start a task on data on the same server or b) immediately start a new task in a farther away place that requires moving data there

What Spark typically does is wait a bit in the hopes that a busy CPU frees up Once that timeout expires it starts moving the data from far away to the free CPU The wait timeout for fallback between each level can be configured individually or all together in one parameter; see the sparklocality parameters on the configuration page for details You should increase these settings if your tasks are long and see poor locality but the default usually works well
his has been a short guide to point out the main concerns you should know about when tuning a Spark application – most importantly data serialization and memory tuning For most programs switching to Kryo serialization and persisting data in serialized form will solve most common performance issues Feel free to ask on the Spark mailing list about other tuning best practices